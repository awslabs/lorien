Lorien: A Unified Infrastructure for Efficient Deep Learning Workloads Delivery
===============================================================================
[![Build Status](https://github.com/comaniac/lorien/workflows/Ubuntu/badge.svg)](https://github.com/comaniac/lorien/actions?query=workflow%3AUbuntu)
[![codecov](https://codecov.io/gh/comaniac/lorien/branch/master/graph/badge.svg?token=0lkUl2RgTa)](https://codecov.io/gh/comaniac/lorien)

Lorien is an infrastructure to massively explore/benchmark the best schedules of given deep learning models.
Since Lorien is deep learning compiler (DLC) agnostic, one can easily implement a Lorien dialect
to support a new DLC.

## Motivation

Although most deep learning compilers (e.g., TVM, Halide) have their own auto-tuning frameworks
to optimize the performance on a certain hardware platform, there are some common challenges for
them to be practically used in production.

1. Tuning Process Scalability and Stability.
Long tuning time affects not only the time-to-market but the stability.
To the best of our knowledge, none of existing auto-tuning frameworks is designed for tuning
on multiple machines, and none of them consider fault tolerance.
The tuning process, hence, has to start over if it was accidentally interrupted.
This is crucial especially on edge devices, which are less reliable than cloud instances
and may fail frequently due to overheat or other factors.

2. Tuning Result Management.
Although almost all auto-tuning frameworks, such as Halide auto-scheduler and AutoTVM,
provide mechanisms to serialize tuning results, all of them use file-based mechanism
and have different formats. As a result, a unified data model is required to manage
tuning results from various of auto-tuning frameworks.

3. Time to Deliver an Efficient Schedule.
Even a database is constructed to serve most user requests, it is still possible that
certain workloads are missing. For example, neural architecture search (NAS) may generate
unseen workloads. This necessitates tuning of the workloads on-the-fly using the auto-tuning
framework. However, modern auto-tuning frameworks usually leverage iterative search algorithms
with on-device measurements, which usually take hours, to find an efficient schedule
for one workload. The unfavorably expensive querying/tuning overhead makes production
deployment impractical.

Accordingly, we design and implement Lorien, an open source system, to address these challenges.
To deal with a large amount of tuning tasks, Lorien provides two distributed mechanisms,
one for cloud platforms and the other for edge devices, with the consideration of scalability,
flexibility, and reliability. The best schedules realized by auto-tuning frameworks are committed
to a NoSQL database with the proposed data model that organizes data for efficient querying.

Please visit the [official documentations](https://comaniac.github.io/lorien) for setup guideline and tutorials.

## System Requirements

* Python 3.6+

* **Amazon DynamoDB (local or aws)**: DynamoDB is used for storing and maintain the tuned schedules.
You can choose to either of the following:

  1. Launch a [local version](https://s3-us-west-2.amazonaws.com/dynamodb-local/dynamodb_local_latest.zip) using JVM on your machine, and specify endpoint URL (e.g. `--db "endpoint_url: http://<your IP>:8000"`) when invoking a tuning procses. 
  
  2. Configure AWS credential on your machine to directly use AWS DynamoDB service. In this case, you do not have to specify any argument in tuning configurations.

* **AWS S3 (optional)**: S3 is used to store the full tuning logs (JSON files generated by AutoTVM). If you specify `--commit-log-to bucket_name` and configure an AWS credential on your machine, then all complete tuning logs will be uploaded to the S3 bucket for debugging or research prupose. Note that this is an optional requirement, so you can ignore the `--commit-log-to` argument if you do not want to keep full tuning logs.

* **AWS Batch (AWS ECR)**: You have to set up AWS batch computation environments, job queues, and job definitions in advance to use Lorien AWS batch worker for tuning. See [this blog post](https://fredhutch.github.io/aws-batch-at-hutch-docs/) for reference. You may also need to build an upload Lorien docker images to AWS ECR as the AWS batch job running container.

## Docker Images

You can directly make use of pre-built Lorien docker images on [Docker Hub](https://hub.docker.com/repository/docker/comaniac0422/lorien/tags), which includes two typs of images for CPU and CPU+CUDA platforms. The docker images have TVM deployed so you can launch a tuning process in the container after cloning Lorien. The docker image is also used for Lorien CI purpose.

## Documentation

[https://comaniac.github.io/lorien/](https://comaniac.github.io/lorien/)
